{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "thorough-dictionary",
      "metadata": {
        "id": "thorough-dictionary"
      },
      "source": [
        "## This retrieves all metadata and statistics for an account - items and collections\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pressed-williams",
      "metadata": {
        "id": "pressed-williams"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7e37a43",
      "metadata": {
        "id": "f7e37a43"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import csv\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "authentic-lending",
      "metadata": {
        "id": "authentic-lending"
      },
      "source": [
        "## Set token and base URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "continued-screen",
      "metadata": {
        "id": "continued-screen"
      },
      "outputs": [],
      "source": [
        "#Set the token in the header.\n",
        "\n",
        "api_call_headers = {'Authorization': 'token ENTER TOKEN'} #example: {'Authorization': 'token dkd8rskjdkfiwi49hgkw...'}\n",
        "\n",
        "\n",
        "#Set the base URL\n",
        "BASE_URL = 'https://api.figshare.com/v2'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "genuine-blowing",
      "metadata": {
        "id": "genuine-blowing"
      },
      "source": [
        "## Retrieve Metadata\n",
        "1. Get basic metadata for account\n",
        "2. Keep the id, title, and date posted\n",
        "3. Call the stats API to retrieve views and downloads for each id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mental-breast",
      "metadata": {
        "id": "mental-breast"
      },
      "outputs": [],
      "source": [
        "#Retrieve list of private metadata- this is for unpublished and published records.\n",
        "#SET THE PAGE SIZE to make sure you get all the records from your account (both public and draft)\n",
        "\n",
        "#Get items owned by account\n",
        "r=requests.get(BASE_URL + '/account/articles?page=1&page_size=50', headers=api_call_headers) \n",
        "articles=json.loads(r.text)\n",
        "\n",
        "if r.status_code != 200:\n",
        "    print('Something is wrong:',r.content)\n",
        "else:\n",
        "    print('Collected',len(articles),'metadata records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hybrid-amendment",
      "metadata": {
        "id": "hybrid-amendment"
      },
      "outputs": [],
      "source": [
        "#Keep records that are either public or fully embargoed (i.e. remove drafts)\n",
        "published_records = []\n",
        "for item in articles:\n",
        "    if item['published_date'] != None: #if a record has a published date\n",
        "           published_records.append(item)\n",
        "            \n",
        "print(len(published_records), \"records kept,\",len(articles) - len(published_records),\"records removed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "common-mainland",
      "metadata": {
        "id": "common-mainland"
      },
      "outputs": [],
      "source": [
        "#Create a dataframe from the JSON formatted data\n",
        "dfbasic = pd.DataFrame(published_records)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "involved-official",
      "metadata": {
        "id": "involved-official"
      },
      "source": [
        "## Collect stats\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac45c32",
      "metadata": {
        "id": "bac45c32"
      },
      "outputs": [],
      "source": [
        "# Create a csv file, use an API to gather data, reopen the csv as a dataframe\n",
        "\n",
        "\n",
        "#Create a list of all the article ids\n",
        "article_ids = [item['id'] for item in published_records]    \n",
        "\n",
        "#Create csv file\n",
        "metadata=open('article-stats'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.csv', 'w', newline='')\n",
        "#Write header row to csv\n",
        "csv.writer(metadata).writerow(['id','views','downloads'])            \n",
        "\n",
        "            \n",
        "for l in article_ids:\n",
        "    s=requests.get('https://stats.figshare.com/total/views/article/'+ str(l))\n",
        "    r=json.loads(s.text)\n",
        "    t=requests.get('https://stats.figshare.com/total/downloads/article/'+ str(l))\n",
        "    q=json.loads(t.text)\n",
        "    \n",
        "    #write the values to the csv file. Dates in json files are seconds from jan 1 1970 so datetime.datetime.fromtimestamp converts\n",
        "    csv.writer(metadata).writerow([\n",
        "        l,\n",
        "        r.get('totals'), #For any of these .get(), adding \",'N/A'\" will fill the null cells with 'N/A'. However, metadata assessment counts non nulls\n",
        "        q.get('totals')]) \n",
        "    \n",
        "    \n",
        "metadata.close() #Close the output file, release all locks\n",
        "\n",
        "#Open up the same file as a dataframe. Encode cp1252 avoids a utf8 error.\n",
        "dfstats = pd.read_csv('article-stats'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.csv',encoding='utf8')\n",
        "\n",
        "print('The resulting dataframe has',len(dfstats),'rows')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "premium-binary",
      "metadata": {
        "id": "premium-binary"
      },
      "source": [
        "### Merge the dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "amino-removal",
      "metadata": {
        "id": "amino-removal"
      },
      "outputs": [],
      "source": [
        "dfmerged = dfbasic.merge(dfstats, how='inner', on='id')\n",
        "dfmerged.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "voluntary-screen",
      "metadata": {
        "id": "voluntary-screen"
      },
      "source": [
        "### If you have Collections run this next cell. Otherwise skip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "discrete-finish",
      "metadata": {
        "id": "discrete-finish"
      },
      "outputs": [],
      "source": [
        "#Retrieve list of private metadata- this is for unpublished and published records.\n",
        "#SET THE PAGE SIZE to make sure you get all the records from your account (both public and draft)\n",
        "\n",
        "#Get items owned by account\n",
        "r=requests.get(BASE_URL + '/account/collections?page=1&page_size=50', headers=api_call_headers) \n",
        "collections=json.loads(r.text)\n",
        "\n",
        "#Keep records that are either public or fully embargoed (i.e. remove drafts)\n",
        "published_coll_records = []\n",
        "for item in collections:\n",
        "    if item['published_date'] != None: #if a record has a published date\n",
        "           published_coll_records.append(item)\n",
        "\n",
        "#Create a dataframe from the JSON formatted data\n",
        "dfcollbasic = pd.DataFrame(published_coll_records)\n",
        "\n",
        "#Gather Stats\n",
        "#Create a list of all the article ids\n",
        "coll_ids = [item['id'] for item in published_coll_records]    \n",
        "\n",
        "#Create csv file\n",
        "metadata=open('collection-stats'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.csv', 'w', newline='')\n",
        "#Write header row to csv\n",
        "csv.writer(metadata).writerow(['id','views','downloads'])            \n",
        "\n",
        "            \n",
        "for l in coll_ids:\n",
        "    s=requests.get('https://stats.figshare.com/total/views/article/'+ str(l))\n",
        "    r=json.loads(s.text)\n",
        "    t=requests.get('https://stats.figshare.com/total/downloads/article/'+ str(l))\n",
        "    q=json.loads(t.text)\n",
        "    \n",
        "    #write the values to the csv file. Dates in json files are seconds from jan 1 1970 so datetime.datetime.fromtimestamp converts\n",
        "    csv.writer(metadata).writerow([\n",
        "        l,\n",
        "        r.get('totals'), #For any of these .get(), adding \",'N/A'\" will fill the null cells with 'N/A'. However, metadata assessment counts non nulls\n",
        "        q.get('totals')]) \n",
        "    \n",
        "    \n",
        "metadata.close() #Close the output file, release all locks\n",
        "\n",
        "#Open up the same file as a dataframe. Encode cp1252 avoids a utf8 error.\n",
        "dfcollstats = pd.read_csv('collection-stats'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.csv',encoding='utf8')\n",
        "\n",
        "dfcollmerged = dfcollbasic.merge(dfcollstats, how='inner', on='id')\n",
        "\n",
        "#Append the collections rows to the article dataframe\n",
        "dfmerged = dfmerged.append(dfcollmerged)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "universal-recipient",
      "metadata": {
        "id": "universal-recipient"
      },
      "source": [
        "### Format the dates colum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thick-preliminary",
      "metadata": {
        "id": "thick-preliminary"
      },
      "outputs": [],
      "source": [
        "#The dates are all contained within one column called 'timeline'. \n",
        "#Use the JSON to create a better format and then merge with the dataframe\n",
        "#with the proper article id in a new dataframe\n",
        "\n",
        "temp_date_list = []\n",
        "\n",
        "for item in published_records:\n",
        "    dateitem = item['timeline']\n",
        "    dateitem['id'] = item['id']\n",
        "    temp_date_list.append(dateitem)\n",
        "\n",
        "df_dates_items = pd.json_normalize(\n",
        "    temp_date_list \n",
        ")\n",
        "\n",
        "\n",
        "#Have to use 'try' here just in case you ran the Collection cell above\n",
        "try:\n",
        "    #Get a dates dataframe\n",
        "    temp_coll_date_list = []\n",
        "\n",
        "    for item in published_coll_records:\n",
        "        dateitem = item['timeline']\n",
        "        dateitem['id'] = item['id']\n",
        "        temp_coll_date_list.append(dateitem)\n",
        "\n",
        "    df_coll_dates_coll = pd.json_normalize(\n",
        "        temp_coll_date_list \n",
        "    )\n",
        "# catch when published_coll_records is None\n",
        "except AttributeError:\n",
        "    pass\n",
        "# catch when it hasn't even been defined\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "\n",
        "#Append the dataframes (if collections have been found)\n",
        "try:\n",
        "    #Append the dates dataframes\n",
        "    df_dates = df_dates_items.append(df_dates_coll)\n",
        "# catch when df_dates_coll is None\n",
        "except AttributeError:\n",
        "    df_dates = df_dates_items\n",
        "# catch when it hasn't even been defined\n",
        "except NameError:\n",
        "    df_dates = df_dates_items\n",
        "    \n",
        "#Merge the date dataframe with the metadata dataframe\n",
        "df_formatted = dfmerged.merge(df_dates, how='outer', on='id')\n",
        "\n",
        "print(\"Dates split out and merged\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "human-means",
      "metadata": {
        "id": "human-means"
      },
      "source": [
        "### View Totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "former-vocabulary",
      "metadata": {
        "id": "former-vocabulary"
      },
      "outputs": [],
      "source": [
        "#See your summarized stats\n",
        "print('Total views =', df_formatted['views'].sum(),'and total downloads=',df_formatted['downloads'].sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "generic-adoption",
      "metadata": {
        "id": "generic-adoption"
      },
      "source": [
        "## Save the spreadsheet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#When you run this cell it will ask you to authenticate so that you can create files to download\n",
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "id": "SA709mKthURp"
      },
      "id": "SA709mKthURp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "df_formatted.to_csv('my-records-'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.csv',encoding='utf-8') #create the CSV\n",
        "files.download('my-records-'+str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))+'.csv') #download to your computer"
      ],
      "metadata": {
        "id": "rPKcZnBTpk_c"
      },
      "id": "rPKcZnBTpk_c",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "ACCOUNT-ITEMS-COLLECTIONS.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}